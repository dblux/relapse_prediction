#!/usr/bin/env python
# coding: utf-8
import numpy as np, pandas as pd
import seaborn as sns
import glob
import re
import matplotlib.pyplot as plt

from sklearn import tree
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression

#%%

rpath_list = sorted(glob.glob("dump/qpsp_bcmD0_PCA/*.tsv"))

# Subtype labels
LABEL_RPATH = "data/GSE67684/processed/metadata_combined-label_subtype_edited.tsv"
subtype_labels = pd.read_csv(LABEL_RPATH, sep="\t")

#%%
# FEATURES_RPATH = "dump/features-qpsp.tsv"
FEATURES_RPATH = "dump/features-quantile.tsv"
# FEATURES_RPATH = "dump/features-cs_quantile.tsv"

data = pd.read_csv(FEATURES_RPATH, sep="\t")
# Feature selection
X = data.iloc[:,np.r_[0,8,10]]
# OPTION 2
# X = data.iloc[:,np.r_[0,8:11]]
list(X)

#%%
### DECISION TREE ###
# for i in range(len(rpath_list)):
#     i = 0
#     print(rpath_list[i])
#     data = pd.read_csv(rpath_list[i], sep="\t")

data = pd.read_csv(FEATURES_RPATH, sep="\t")

# Feature selection
X = data.iloc[:,np.r_[0,8:11]]
print(list(X))
print("X.shape =", X.shape)
# Create labels
y = data.iloc[:,15]

# Create and train model
clf = tree.DecisionTreeClassifier(random_state=0, max_depth=3)
clf = clf.fit(X, y)
# Results of training set
print(clf.score(X,y))

# Decision rules
tree_rules = tree.export_text(clf, feature_names=list(X))
print(tree_rules)


# In[567]:


subtype = subtype_labels.loc[X.index.values, "subtype"]
# Concatenate subtype and truth labels
X1 = pd.concat([X, subtype, y],1).sort_values(["erm1","l2norm_d0_d8"])
list_dfs = list(X1.groupby("subtype"))
X5 = list_dfs[7][1]
print(list_dfs[7][0])
X2 = X5[X5["erm1"] < 74]
# X3 = X2[X2["l2norm_d0_d8"] > 0.54]
# X4 = X3[X3["angle_d0_d8"] > 49]
# X2.sort_values("l2norm_d0_d8")
# X1[X1["label"] == 1]

X2.sort_values("l2norm_d0_d8")


# In[295]:


# Construct decision tree
X2 = X1[X1["erm1"] < 0.8]
X3 = X2[X2["l2norm_d0_d8"] < 1.96]
X4 = X3[X3["angle_d0_d8"] > 49]
X4

# Random stratified split
# X_train, X_test, y_train, y_test = train_test_split(X, y,
#                                                     random_state=1,
#                                                     test_size=0.2)


# In[292]:


get_ipython().run_line_magic('matplotlib', 'notebook')
# Visualise rules of decision tree
decision_plot = tree.plot_tree(clf, feature_names=list(X),
               class_names=("remission", "relapse"), filled=True)


# In[467]:


i = 4
print(rpath_list[i])
data = pd.read_csv(rpath_list[i], sep="\t")

# Feature selection
X = data.iloc[:,np.r_[0,8:11]]
print("X.shape =", X.shape)
print(list(X))
# Create labels
y = data.iloc[:,15]

# X1 = pd.concat([X,y],1).sort_values(["erm1","l2norm_d0_d8"])
X1 = pd.concat([X,y],1).sort_values(["erm1"])
X1


# In[473]:


# Construct decision tree
X2 = X1[X1["erm1"] < 0.8]
X3 = X2[X2["l2norm_d0_d8"] < 1.96]
X4 = X3[X3["angle_d0_d8"] > 49]
X4


# In[430]:


y_predict = data["d33_mrd"] > 0.0001
y_predict = y_predict.astype(int)

sum(y_predict != y)
pd.concat([y_predict, data.iloc[:,14:16]], 1)


#%%
### NAIVE BAYES ###

# Import data
rpath_list = sorted(glob.glob("dump/qpsp_bcmD0_PCA/*.tsv"))

# OPTION 1
FEATURES_RPATH = rpath_list[4]
# OPTION 2
#FEATURES_RPATH = "dump/features1-T-ALL.tsv"

data = pd.read_csv(FEATURES_RPATH, sep="\t")
SUBTYPE = re.search("features-(.+?).tsv", FEATURES_RPATH).groups()[0]
print(FEATURES_RPATH)
print(SUBTYPE)
print(data.shape)

# Feature selection
X1 = data.iloc[:,np.r_[0,8,10]]
print("X1:", list(X1))
# OPTION 2
X2 = data.iloc[:,np.r_[0,8,6]]
print("X2:", list(X2))

y = data.iloc[:,15]

#%%

gnb_clf = GaussianNB()

# Fit to X1
gnb_clf.fit(X1, y)
# Fitted parameters for each class
class_var1 = gnb_clf.sigma_
class_mean1 = gnb_clf.theta_
# Prediction
y_pred = gnb_clf.predict(X1)
score1 = "X1: {}/{}".format((y_pred == y).sum(), len(y))

# Fit to X2
gnb_clf.fit(X2, y)
# Fitted parameters for each class
class_var2 = gnb_clf.sigma_
class_mean2 = gnb_clf.theta_
# Prediction
y_pred = gnb_clf.predict(X2)
score2 = "X2: {}/{}".format((y_pred == y).sum(), len(y))

#%%
# Plot features
data1 = pd.concat([data.iloc[:,[0,8,10,6]], y.astype("category")], 1)
data1_long = data1.melt(id_vars=["label"], var_name="feature")

fig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1, figsize=(5,8))
fig.subplots_adjust(hspace=0.5)
fig.suptitle("{} ({}; {})".format(SUBTYPE, score1, score2))

sns.stripplot(x="erm1", y="label", data=data1, ax=ax1)
sns.stripplot(x="l2norm_d0_d8", y="label", data=data1, ax=ax2)
sns.stripplot(x="angle_d0_d8", y="label", data=data1, ax=ax3)
sns.stripplot(x="d0_normal_proj", y="label", data=data1, ax=ax4)

#ax1.axvline(class_mean1[0,0]); ax1.axvline(class_mean1[1,0], color="orange")
#ax2.axvline(class_mean1[0,1]); ax2.axvline(class_mean1[1,1], color="orange")
#ax3.axvline(class_mean1[0,2]); ax3.axvline(class_mean1[1,2], color="orange")
#ax4.axvline(class_mean2[0,2]); ax4.axvline(class_mean2[1,2], color="orange")

ax1.axvline(0.8, color="r")
ax2.axvline(1, color="r"); ax2.axvline(1.96, color="g")
ax3.axvline(49, color="g")
ax4.axvline(2.6, color="r")


fig.savefig("dump/strplt_threshold-{}.pdf".format(SUBTYPE))

#%%

X1_y = pd.concat([X2,y], 1)
X4 = X1_y[X1_y["erm1"] < 0.8]
print(X4)
X5 = X4[X4["l2norm_d0_d8"] < 1.96]
print(X5)

#%%
##### LOGISTIC REGRESSION #####
### Likelihood ratios from GNB ###
lr = pd.read_csv("dump/telaml1-lr.tsv", sep="\t")
X = lr.iloc[:,0:4]
y = lr.iloc[:,5]

# Create new instance of logistic regression classifier
logreg = LogisticRegression(random_state=0, solver="lbfgs")
logreg.fit(X,y)
logreg.score(X,y)

logreg.predict(X)
proba = logreg.predict_proba(X)

# Attributes of the classifier
beta = logreg.coef_
beta_0 = logreg.intercept_
# np.sum sums ALONG the axis
# Before logistic function
X_beta = np.sum(X*beta, axis = 1) + beta_0
# Prediction tallies with coefficients
pred_class1 = 1/(1+np.exp(-X_beta))

plt.scatter(np.arange(39), pred_class1, c = y+1)

### Original features ###
# Import features
telaml1_features = pd.read_csv("dump/qpsp_bcmD0_PCA/features-TEL-AML1.tsv",
                               sep="\t")
list(telaml1_features)
X_selected = telaml1_features.iloc[:,[0,6,8,10,14]]
# OPTION 2: No MRD33
X_selected1 = telaml1_features.iloc[:,[0,6,8,10]]
list(X_selected1)

logreg1 = LogisticRegression(random_state=0, solver="lbfgs")
logreg1.fit(X_selected1,y)
logreg1.score(X_selected1,y)
proba1 = logreg1.predict_proba(X_selected1)

# Attributes of the classifier
beta = logreg.coef_
beta_0 = logreg.intercept_
# np.sum sums ALONG the axis
# Before logistic function
X_beta = np.sum(X*beta, axis = 1) + beta_0
# Prediction tallies with coefficients
pred_class1 = 1/(1+np.exp(-X_beta))

# EDA
plt.scatter(np.arange(39), proba[:,1], c = y+1)
plt.scatter(np.arange(39), proba1[:,1], c = y+1)

### Global GSS ###
csquantile_features = pd.read_csv("dump/features-cs_quantile.tsv", sep="\t")
